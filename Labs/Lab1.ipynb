{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Lab1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNWo2X4HiBKR"
      },
      "source": [
        "# Assingment 1 - Malaria Cell Image Classification\n",
        "### Course: Convolutional Neural Networks with Applications in Medical Image Analysis\n",
        "\n",
        "Office hours: Alternating weeks on Thursdays 13.15--16.00 (Minh) and Wednesdays 08.15--12.00 (Attila). See the course web page for details and potential changes.\n",
        "\n",
        "The first assignment is based on classifying images of cells, whether they are parasitized or uninfected by malaria. Your input will be an image of a cell, and your output is a binary classifier. It is based on an open dataset, available from Lister Hill National Center for Biomedical Communications (NIH): https://lhncbc.nlm.nih.gov/publication/pub9932. The file is available online through ftp://lhcftp.nlm.nih.gov/Open-Access-Datasets/Malaria/cell_images.zip but it's also available through a github repository (https://github.com/attilasimko/CNN2021.git) which can be directly linked to your colab *IF* you have a Google Drive account. If that is not the case, refer to the assignment instructions that detail using the CS lab computers and try again with jupyter-notebook (very similar to colab). The data has been preprocessed and organized for easier machine learning application.\n",
        "\n",
        "Your task is to look through the highly customizable code below, which contains all the main steps for high accuracy classification of these data, and improve upon the model. The most important issues with the current code are noted in the comments for easier comprehension. Your tasks, to include in the report, are:\n",
        "\n",
        "- Reach an accuracy of at least 96~\\% on the validation dataset.\n",
        "- Plot the training/validating losses and accuracies. Describe when to stop training, and why that is a good choice.\n",
        "- Describe your thought process behind building the model and choosing the model's hyper-parameters.\n",
        "- Describe what you think are the biggest issues with the current setup, and how to solve them.\n",
        "\n",
        "Upload the updated notebook to canvas, and make sure \n",
        "The deadline for the assignment is February $25^{th}$, 15:00.\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOstKZnmiBKs"
      },
      "source": [
        "# Import necessary packages for loading the dataset\n",
        "\n",
        "import numpy as np  # Package for matrix operations, handling data\n",
        "np.random.seed(2020)\n",
        "import os\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt  # Package for plotting\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow\n",
        "\n",
        "# Setup the GPU, if you have one on the computer you are running on\n",
        "gpus = tensorflow.config.experimental.list_physical_devices('GPU')\n",
        "if len(gpus) > 0:\n",
        "    tensorflow.config.experimental.set_memory_growth(gpus[0], True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drNAKk4DiUlv"
      },
      "source": [
        "The data used for the assignment can be found on https://github.com/attilasimko/CNN2021.git\r\n",
        "\r\n",
        "If you have a google drive account, you can use the github repo directly through colab. If you don't have one refer to the assignment instructions for using the CS computers with jupyter-notebook.\r\n",
        "\r\n",
        "If you decide to use google colab instead of working from the lab computers, set the variable \"lab_computer\" to False. This will download the necessary files from a github repository to your google drive (~300MB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWuqJccNjI1T"
      },
      "source": [
        "lab_computer = True\r\n",
        "if (lab_computer):\r\n",
        "  data_path = '/import/software/3ra023vt21/cell_images/'\r\n",
        "else:\r\n",
        "  if (not(os.path.isdir('/content/CNN2021'))):\r\n",
        "    ! git clone https://github.com/attilasimko/CNN2021.git\r\n",
        "  data_path = '/content/CNN2021/LAB1/'\r\n",
        "  %cd /content/CNN2021/LAB1/\r\n",
        "  ! git pull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ9gg0LcrKtQ"
      },
      "source": [
        "If you get any popup windows from google drive regarding loading your data, just follow their instructions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQW_5B2UiBKv"
      },
      "source": [
        "# Check out dataset\n",
        "parasitized_data = os.listdir(data_path + 'Parasitized/')\n",
        "print(parasitized_data[:2])  # the output we get are the .png files\n",
        "print(\"Number of parasitized images: \" + str(len(parasitized_data)) + '\\n')\n",
        "uninfected_data = os.listdir(data_path + 'Uninfected/')\n",
        "print(uninfected_data[:2])\n",
        "print(\"Number of non-paratisized images: \" + str(len(uninfected_data)))\n",
        "\n",
        "# NOTE: The images are in .png format, they will have to be loaded individually and handled accordingly."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0ol5-jOiBKy"
      },
      "source": [
        "# Look at some sample images\n",
        "plt.figure(figsize=(12, 5))\n",
        "for i in range(4):\n",
        "    plt.subplot(1, 4, i + 1)\n",
        "    img = plt.imread(data_path + '/Parasitized/' + parasitized_data[i])\n",
        "    plt.imshow(img)\n",
        "    plt.title('Image size: ' + str(np.shape(img)))\n",
        "    plt.tight_layout()\n",
        "\n",
        "plt.suptitle('Parasitized Image Samples')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i in range(4):\n",
        "    plt.subplot(1, 4, i + 1)\n",
        "    img = plt.imread(data_path + '/Uninfected/' + uninfected_data[i + 1])\n",
        "    plt.imshow(img)\n",
        "    plt.title('Image size: ' + str(np.shape(img)))\n",
        "    plt.tight_layout()\n",
        "\n",
        "plt.suptitle('Uninfected Image Samples')\n",
        "plt.show()\n",
        "\n",
        "# NOTE: The images are of different size. Also they are RGB images."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQaDl4-hiBKz"
      },
      "source": [
        "The dataset preprocessing so far has been to help you, you should not change anything. However, from now on, take nothing for granted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeWCirCciBK0"
      },
      "source": [
        "# Create dataset for machine learning purposes.\n",
        "\n",
        "print(\"Loading data, this may take a while ...\")\n",
        "\n",
        "height = 16\n",
        "width = 16\n",
        "# NOTE: The size of the new images is very important.\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "for img in parasitized_data:\n",
        "    try:\n",
        "        img_read = plt.imread(data_path + '/Parasitized/' + \"/\" + img)\n",
        "        img_resize = resize(img_read, (height, width))\n",
        "        img_array = img_to_array(img_resize)\n",
        "        data.append(img_array)\n",
        "        labels.append(1)\n",
        "    except:\n",
        "        None\n",
        "\n",
        "for img in uninfected_data:\n",
        "    try:\n",
        "        img_read = plt.imread(data_path + '/Uninfected' + \"/\" + img)\n",
        "        img_resize = resize(img_read, (height, width))\n",
        "        img_array = img_to_array(img_resize)\n",
        "        data.append(img_array)\n",
        "        labels.append(0)\n",
        "    except:\n",
        "        None\n",
        "\n",
        "print(\"Done!\")\n",
        "\n",
        "# NOTE: The labels are 1 if the corresponding image is paratisized, 0 if not."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm-mm8YWiBK1"
      },
      "source": [
        "image_data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Shuffle data\n",
        "idx = np.arange(image_data.shape[0])\n",
        "np.random.shuffle(idx)\n",
        "image_data = image_data[idx]\n",
        "labels = labels[idx]\n",
        "\n",
        "# Sizes of datasets\n",
        "print(f\"Input data shape : {np.shape(image_data)}\")\n",
        "print(f\"Output data shape: {np.shape(labels)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORls6CGniBK1"
      },
      "source": [
        "# Split dataset into training and testing dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(image_data, labels, test_size=0.2, random_state=101)\n",
        "# NOTE: Keep the ratio of the split as it is. It will make evaluation easier for us. \n",
        "# NOTE: The split should be reproducible, hence the random state.\n",
        "\n",
        "# Split testing dataset into testing and validation\n",
        "x_test, x_val = x_test[0:int(len(x_test) / 2), :], x_test[int(len(x_test) / 2):, :]\n",
        "y_test, y_val = y_test[0:int(len(y_test) / 2)], y_test[int(len(y_test) / 2):]\n",
        "\n",
        "# Two samples from the testing set for heatmapping\n",
        "x_call, y_call = x_test[[np.argmin(y_test), np.argmax(y_test)]], y_test[[np.argmin(y_test), np.argmax(y_test)]]\n",
        "\n",
        "# NOTE: Pick one parasitized and one uninfected too."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzgUVtZeiBK2"
      },
      "source": [
        "plt.subplot(121)\n",
        "plt.imshow(x_call[0, :, :, :])\n",
        "plt.title(\"Uninfected\")\n",
        "plt.subplot(122)\n",
        "plt.imshow(x_call[1, :, :, :])\n",
        "plt.title(\"Infected\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOeZxM1hiBK4"
      },
      "source": [
        "# Make the labels keras-friendly\n",
        "y_train = to_categorical(y_train, num_classes=2)\n",
        "y_val = to_categorical(y_val, num_classes=2)\n",
        "y_test = to_categorical(y_test, num_classes=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjuarJEZiBK4"
      },
      "source": [
        "# A quick summary of the data:\n",
        "print(f\"Training image size  : {str(x_train.shape)}\")\n",
        "print(f\"Validation image size: {str(x_val.shape)}\")\n",
        "print(f\"Testing image size   : {str(x_test.shape)}\")\n",
        "print(\"\")\n",
        "print(f\"Training label size  : {str(y_train.shape)}\")\n",
        "print(f\"Validating label size: {str(y_val.shape)}\")\n",
        "print(f\"Testing label size   : {str(y_test.shape)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEqTfhPJiBK5"
      },
      "source": [
        "# Import packages important for building and training your model.\n",
        "\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.layers import Dense, Conv2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LKQ0WIgiBK6"
      },
      "source": [
        "def build_model(height, width, classes, channels):\n",
        "    model = Sequential()\n",
        "    inputShape = (height, width, channels)\n",
        "    chanDim = -1\n",
        "\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        inputShape = (channels, height, width)\n",
        "\n",
        "    model.add(Conv2D(8, (4, 4), strides=(2, 2), input_shape=inputShape, name='hotmap'))\n",
        "    model.add(Conv2D(8, (1, 1), strides=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(classes, activation='relu'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# NOTE: Look at the imported layers in the previous cell. Feel free to use all of them.\n",
        "# NOTE: Are you satisfied with the model being sequential? Feel free to experiment.\n",
        "# NOTE: The first activation layer is named 'hotmap' for further use for heatmapping.\n",
        "# NOTE: What activation are you using on the output layer? What range will your output have?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICD3NY3siBK7"
      },
      "source": [
        "# Build your model.\n",
        "\n",
        "classes = 2\n",
        "channels = 3\n",
        "model = build_model(height=height, width=width, classes=classes, channels=channels)\n",
        "print(f'Size of the model input: {str(model.input.shape)}')\n",
        "model.summary()\n",
        "\n",
        "# NOTE: Are the input sizes correct?\n",
        "# NOTE: Are the output sizes correct?\n",
        "# NOTE: Is the 'hotmap' activation layer in the model?\n",
        "# NOTE: Try to imagine the model layer-by-layer and think it through. Is it doing something reasonable?\n",
        "# NOTE: Are the model parameters split \"evenly\" between the layers? Or is there one huge layer?\n",
        "# NOTE: Will the model fit into memory? Is the model too small? Is the model too large?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4slF82miBK8"
      },
      "source": [
        "# Compile the model. Do try out different optimizers, learning rates, etc. to see if you can improve the results\n",
        "learning_rate = 0.01\n",
        "optim = optimizers.SGD(lr=learning_rate)\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=optim,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# NOTE: Are you satisfied with the loss function?\n",
        "# NOTE: Are you satisfied with the metric?\n",
        "# NOTE: Are you satisfied with the optimizer and its parameters?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrhMkf5riBK9"
      },
      "source": [
        "#fit the model onto the dataset\n",
        "batch_size = 2048\n",
        "n_epochs = 20\n",
        "\n",
        "h = model.fit(x_train, y_train, epochs=n_epochs, batch_size=batch_size)\n",
        "\n",
        "# NOTE: Plotting the accuracies and losses helps a lot.\n",
        "# NOTE: What does plotting the training data tell you? Should you plot something else?\n",
        "# NOTE: What should one do with the validation data?\n",
        "# NOTE: When should one stop? Did you overtrain? Did you train for long enough?\n",
        "# NOTE: Think about implementing Early Stopping?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7um34_baiBK-"
      },
      "source": [
        "# Grad-GAM heatmaps\n",
        "# Note: You need to install tf-explain for this to work\n",
        "try:\n",
        "    from tf_explain.core.grad_cam import GradCAM\n",
        "\n",
        "    class_index = 1\n",
        "    explainer = GradCAM()\n",
        "    # Compute GradCAM\n",
        "    grid = explainer.explain((x_call, y_call), model, class_index=class_index, layer_name=\"hotmap\")\n",
        "    plt.imshow(np.sum(grid, axis=2), cmap='bwr')\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "    # NOTE: We look at the activation function of the layer called 'hotmap' from the model.\n",
        "    # NOTE: What does this image mean?\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKbsn8rWiBK_"
      },
      "source": [
        "# Evaluate the model on the validation data. Its this value that should exceed 0.96 (i.e., 95 %).\n",
        "predictions = model.evaluate(x_val, y_val, verbose=0)\n",
        "print(f\"Validation loss    : {predictions[0]:.3f}\")\n",
        "print(f\"Validation accuracy: {predictions[1]:.3f}\")\n",
        "\n",
        "# NOTE: Is this high enough? How about varying model hyper-parameters? Perhaps implement data augmentation?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrjIxlNdiBLA"
      },
      "source": [
        "# Final model's test error. Note that you should _NOT_ look at this value until you are\n",
        "# completely happy with your model. Report this value in the report.\n",
        "if False:\n",
        "    predictions = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"Test loss    : {predictions[0]:.3f}\")\n",
        "    print(f\"Test accuracy: {predictions[1]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}